# -*- coding: utf-8 -*-
"""tarefa de casa valcann.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rt4VnguGm3earCTOKCLWKiRd2OGpIgr5

1. fazer csv com tudo -----------------------------------------------------------------------------------
2. fazer script para atualizar csv's (tanto o pruduzido como o com tudo)----
3. fazer script para upload em horario especifico-----------------------------------------
4. usar airflow para gerenciar jobs------------------------------------------------------------------
5. reescrever codigo de forma bonitinha--------------------------------------------------------------
6. upload no git-------------------------------------------------------------------------------------------
7. escrever readme--------------------------------------------------------------------------------
8. bonus: paralelizar------------------------------------------------------------------------------------
9. bonus: fazer script n√£o parar---------------------------------------------

#Scrapping
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import numpy as np
import pandas as pd

# only run if you have installed Selenium on your local computer!
import selenium.webdriver
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException

driver = webdriver.Chrome(ChromeDriverManager().install())

driver.get("http://books.toscrape.com/catalogue/category/books_1/page-1.html")

html_soup = BeautifulSoup(driver.page_source, 'html.parser')

page_num_text = html_soup.find('li', class_ ='current').text

page_num = int(max(re.findall(r'\d+', page_num_text)))

# Lets find all books in the page
incategory = driver.find_elements_by_class_name("product_pod")
#Generate a list of links for each and every book
links = []
for i in range(len(incategory)):
	item = incategory[i]
	#get the href property
	a = item.find_element_by_tag_name("h3").find_element_by_tag_name("a").get_property("href")
	#Append the link to list links
	links.append(a)

#Generate a list of links for each and every book
links = []
page_counter = 0

while (True):
	# Lets find all books in the page
	incategory = driver.find_elements_by_class_name("product_pod")
	for i in range(len(incategory)):
		item = incategory[i]
		#get the href property
		a = item.find_element_by_tag_name("h3").find_element_by_tag_name("a").get_property("href")
		#Append the link to list links
		links.append(a)
	
	soup = BeautifulSoup(driver.page_source, 'html.parser')
	timeout = 5
	try:
		element_present = EC.presence_of_element_located((By.ID, 'main'))
		WebDriverWait(driver, timeout).until(element_present)
	except TimeoutException:
		print("Timed out waiting for page to load")
	finally:
		print("Page loaded")
	page_counter = page_counter + 1
	if (page_counter < page_num):
		click_next = driver.find_element(By.CSS_SELECTOR, '.next > a:nth-child(1)')
		click_next.click()
	else:
		break

data = []
# Lets loop through each link to acces the page of each book
for link in links:
	# get one book url
	driver.get(url=link)
	# title of the book
	title = driver.find_element_by_xpath("//*[@id='content_inner']/article/div[1]/div[2]/h1")
	# price of the book
	price = driver.find_element_by_xpath("//*[@id='content_inner']/article/div[1]/div[2]/p[1]")
	# stock - number of copies available for the book
	stock = driver.find_element_by_xpath("//*[@id='content_inner']/article/div[1]/div[2]/p[2]")
	# Stock comes as string
	stock = int(re.findall("\d+",stock.text)[0])
	# Stars - Actual stars are in the tag attribute
	stars = driver.find_element_by_xpath("//*[@id='content_inner']/article/div[1]/div[2]/p[3]").get_attribute("class")
	# Category of the book
	category_a =  driver.find_element_by_xpath("//*[@id='default']/div/div/ul/li[3]/a")

	# Define a dictionary with details we need
	r = {
		"Title":title.text,
		"Category":category_a.text,
		"Stock": stock,
		"Stars": stars,
		"Price":price.text
	}
	# append r to all details
	data.append(r)


time.sleep(4)

pd.DataFrame(data).to_csv('books.csv', index=False, sep='\t')